{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mining project using data from Reddit.\n",
    "\n",
    "1 - Detect subreddits that act as echo chambers in Reddit\n",
    "\n",
    "2 - Find the main topics discussed in those subreddits\n",
    "\n",
    "3 - Find subreddits with opposing views and analyze the sentiment towards each topic\n",
    "\n",
    "4 - Finally, I want to measure if users change the sentiment, their vocabulary or their opinion\n",
    "\n",
    "on a topic based on the subreddit they are interacting with. I also would to see if\n",
    "spending a greater deal of time on a subreddit that acts as an echo chamber changes\n",
    "their sentiment from negative/neutral to positive (or vice versa) on specific topics.\n",
    "\n",
    "List of methods that I need to apply in the project:\n",
    "1. tf-idf\n",
    "2. show the most frequent words. Use wordcloud\n",
    "3. correlation between words (network)\n",
    "4. topic modeling and connect topics with the most frequent words\n",
    "5. sentiment analysis\n",
    "6. Maybe cluster analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, json, os, re, zstandard\n",
    "from nltk.corpus import stopwords\n",
    "from zst_reader import read_lines_zst, write_line_zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\"Conservative\", \"progressive\",\n",
    "              \"democrats\", \"Republican\",\n",
    "              \"NeutralPolitics\", \"PoliticalDiscussion\", \"politics\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detecting Echo Chambers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Retrieve Reddit data\n",
    "\n",
    "Posts: Retrieve the following information for each post:\n",
    "* Title: The title of the post.\n",
    "* Content: The text content of the post.\n",
    "* Upvotes and downvotes: The number of upvotes and downvotes received by the post.\n",
    "* Timestamp: The date and time when the post was created.\n",
    "\n",
    "Comments: Retrieve the following information for each comment:\n",
    "* Content: The text content of the comment.\n",
    "* Upvotes and downvotes: The number of upvotes and downvotes received by the comment.\n",
    "* Timestamp: The date and time when the comment was posted.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms frequencies (unigram and bigram) in the comments\n",
    "def count_terms_frequency(input_comments: list, output_frequencies: list) -> None:\n",
    "\n",
    "    # Load stop words using nltk\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Loop through input paths\n",
    "    for in_comment, out_grams in zip(input_comments, output_frequencies):\n",
    "\n",
    "        unigrams = Counter()\n",
    "        bigrams = Counter()\n",
    "\n",
    "        for line, file_bytes_processed in read_lines_zst(in_comment):\n",
    "\n",
    "            # Load the json object\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Skip if body doesn't exist\n",
    "            if 'body' not in obj:\n",
    "                continue\n",
    "\n",
    "            # Get body of comment\n",
    "            body = obj['body']\n",
    "\n",
    "            # Skip if body is deleted or removed\n",
    "            if (body == 'deleted') or (body == 'removed'):\n",
    "                continue\n",
    "\n",
    "            # Clean the text\n",
    "            body = clean_comments(body, stop_words)\n",
    "\n",
    "            # Split the text into unigrams and bigrams\n",
    "            unigrams_list = body.split()\n",
    "            bigrams_list = list(ngrams(unigrams_list, 2))\n",
    "\n",
    "            unigrams.update(unigrams_list)\n",
    "            bigrams.update(bigrams_list)\n",
    "\n",
    "        # Create the zst handler\n",
    "        handle_unigram = zstandard.ZstdCompressor().stream_writer(open(out_grams[0], 'wb'))\n",
    "\n",
    "        # Write the unigrams to the zst file\n",
    "        for unigram in unigrams:\n",
    "            line = {'term': unigram, 'frequency': unigrams[unigram]}\n",
    "            line = json.dumps(line)\n",
    "            write_line_zst(handle_unigram, line)\n",
    "\n",
    "        # Create the zst handler\n",
    "        handle_bigram = zstandard.ZstdCompressor().stream_writer(open(out_grams[1], 'wb'))\n",
    "\n",
    "        # Write the bigrams to the zst file\n",
    "        for bigram in bigrams:\n",
    "            line = {'term': bigram, 'frequency': bigrams[bigram]}\n",
    "            line = json.dumps(line)\n",
    "            write_line_zst(handle_bigram, line)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comments = [f\"data/{s}/{s}_comments_clean.zst\" for s in subreddits]\n",
    "output_frequencies = [(f\"analysis/1a/{s}_comments_unigrams.zst\", f\"analysis/1a/{s}_comments_bigrams.zst\") for s in subreddits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_terms_frequency(input_comments, output_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unigrams\n",
    "unigrams = {}\n",
    "for subreddit in subreddits:\n",
    "    unigrams[subreddit] = {}\n",
    "    for line, file_bytes_processed in read_lines_zst(f\"analysis/1a/{subreddit}_comments_unigrams.zst\"):\n",
    "        obj = json.loads(line)\n",
    "        unigrams[subreddit][obj[\"term\"]] = obj[\"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words = [re.sub(\"'\", \"\", word) for word in stop_words]\n",
    "stop_words += [\"like\", \"ever\", \"ive\", \"always\", \"final\", \"people\", \"would\", \"rrepublican\", \"rneutralpolitics\", \"get\", \"one\", \"thats\", \"trump\", \"karma\", \"said\"]\n",
    "# Function to clean unigrams\n",
    "def clean_unigrams(unigrams: dict, min_frequency: int=10, max_frequency: int=1000) -> dict:\n",
    "\n",
    "    # Strip whitespace\n",
    "    unigrams = {k.strip(): v for k, v in unigrams.items()}\n",
    "    \n",
    "    # Remove stop words\n",
    "    unigrams = {k: v for k, v in unigrams.items() if k not in stop_words}\n",
    "    \n",
    "    # Remove terms with frequency less than min_frequency\n",
    "    unigrams = {k: v for k, v in unigrams.items() if v >= min_frequency}\n",
    "\n",
    "    # Remove terms with frequency more than max_frequency\n",
    "    unigrams = {k: v for k, v in unigrams.items() if v <= max_frequency}\n",
    "\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_clean = {}\n",
    "for subreddit in subreddits:\n",
    "    unigrams_clean[subreddit] = clean_unigrams(unigrams[subreddit], min_frequency=10, max_frequency=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unigram frequencies for each subreddit\n",
    "unigrams_frequencies = {}\n",
    "for subreddit in subreddits:\n",
    "    total = sum(unigrams[subreddit].values())\n",
    "    unigrams_frequencies[subreddit] = {k: v/total for k, v in unigrams[subreddit].items()}\n",
    "\n",
    "    # Remove stop words\n",
    "    unigrams_frequencies[subreddit] = {k: v for k, v in unigrams_frequencies[subreddit].items() if k not in stop_words}\n",
    "\n",
    "    # Remove terms with frequency in the top 1%\n",
    "    unigrams_frequencies[subreddit] = {k: v for k, v in unigrams_frequencies[subreddit].items() if v < 0.002}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a word cloud for each subreddit using the top 100 unigrams\n",
    "for subreddit in subreddits:\n",
    "\n",
    "    # Create the word cloud\n",
    "    wc = WordCloud(background_color=\"white\", max_words=150, width=800, height=400)\n",
    "    wc.generate_from_frequencies(unigrams_clean[subreddit])\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for {subreddit}\")\n",
    "    #plt.show()\n",
    "\n",
    "    # Save the image\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f\"analysis/1a/plots/{subreddit}_comments_2_wordcloud.png\")\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart of the 20 most frequent unigrams for each subreddit\n",
    "for subreddit in subreddits:\n",
    "    \n",
    "    # Get the top 20 unigrams\n",
    "    top_20_unigrams = dict(sorted(unigrams_clean[subreddit].items(), key=lambda item: item[1], reverse=True)[:20])\n",
    "\n",
    "    # Plot the bar chart horizontally using seaborn. Use yellowgreen as the color with gradient\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.barplot(x=list(top_20_unigrams.values()), y=list(top_20_unigrams.keys()), palette=\"YlGnBu_r\")\n",
    "\n",
    "    # Set the title and axis labels\n",
    "    plt.title(f\"Top 20 Unigrams for {subreddit}\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Unigram\")\n",
    "\n",
    "    # Save the image\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f\"analysis/1a/plots/{subreddit}_comments_clean_unigrams_top20.png\")\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unigram frequency between Republican and Democrat subreddits of top 100 unigrams\n",
    "# Create a dataframe of the top 100 unigrams for each subreddit\n",
    "df_unigrams = pd.DataFrame(unigrams_frequencies)\n",
    "df_unigrams = df_unigrams.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conservative</th>\n",
       "      <th>progressive</th>\n",
       "      <th>democrats</th>\n",
       "      <th>Republican</th>\n",
       "      <th>NeutralPolitics</th>\n",
       "      <th>PoliticalDiscussion</th>\n",
       "      <th>politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>die</th>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>2.248652e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>5.463000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afford</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.656714e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthcare</th>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>4.594411e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aca</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>7.595825e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictespecially</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looksa</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massprivatizationof</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falsedillemma</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discussionoff</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2389907 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Conservative  progressive  democrats  Republican   \n",
       "die                      0.000354     0.000349   0.000297    0.000294  \\\n",
       "able                     0.000597     0.000572   0.000522    0.000486   \n",
       "afford                   0.000153     0.000256   0.000134    0.000125   \n",
       "healthcare               0.000283     0.000585   0.000412    0.000274   \n",
       "aca                      0.000015     0.000115   0.000081    0.000016   \n",
       "...                           ...          ...        ...         ...   \n",
       "predictespecially        0.000000     0.000000   0.000000    0.000000   \n",
       "looksa                   0.000000     0.000000   0.000000    0.000000   \n",
       "massprivatizationof      0.000000     0.000000   0.000000    0.000000   \n",
       "falsedillemma            0.000000     0.000000   0.000000    0.000000   \n",
       "discussionoff            0.000000     0.000000   0.000000    0.000000   \n",
       "\n",
       "                     NeutralPolitics  PoliticalDiscussion      politics  \n",
       "die                         0.000089             0.000216  2.248652e-04  \n",
       "able                        0.000614             0.000690  5.463000e-04  \n",
       "afford                      0.000132             0.000161  1.656714e-04  \n",
       "healthcare                  0.000296             0.000553  4.594411e-04  \n",
       "aca                         0.000073             0.000129  7.595825e-05  \n",
       "...                              ...                  ...           ...  \n",
       "predictespecially           0.000000             0.000000  2.098700e-09  \n",
       "looksa                      0.000000             0.000000  2.098700e-09  \n",
       "massprivatizationof         0.000000             0.000000  2.098700e-09  \n",
       "falsedillemma               0.000000             0.000000  2.098700e-09  \n",
       "discussionoff               0.000000             0.000000  2.098700e-09  \n",
       "\n",
       "[2389907 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatterplot of unigram frequency between two subreddits (select top 100 unigrams)\n",
    "s_1 = \"democrats\"\n",
    "s_2 = \"politics\"\n",
    "\n",
    "# Create the scatterplot\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(df_unigrams[s_1], df_unigrams[s_2], alpha=0.5)\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title(f\"Unigram Frequency Comparison between {s_1} and {s_2}\")\n",
    "plt.xlabel(f\"Frequency of {s_1}\")\n",
    "plt.ylabel(f\"Frequency of {s_2}\")\n",
    "\n",
    "# Anotate the top 10 unigrams\n",
    "for i, unigram in enumerate(df_unigrams.index):\n",
    "    if i < 10:\n",
    "        plt.annotate(unigram, (df_unigrams.loc[unigram, s_1], df_unigrams.loc[unigram, s_2]),\n",
    "                     fontweight=\"bold\", backgroundcolor=\"white\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Add a line with slope 1\n",
    "plt.plot([0, 0.002], [0, 0.002], color=\"grey\", linestyle=\"--\")\n",
    "\n",
    "plt.savefig(f\"analysis/1a/plots/{s_1}_{s_2}_unigram_frequency_comparison.png\")\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unigram frequency between Republican and Democrat subreddits, and between Conservative and Progressive subreddits\n",
    "for subreddit1, subreddit2 in [(\"Republican\", \"democrats\"), (\"Conservative\", \"progressive\")]:\n",
    "\n",
    "    # Get the top 20 unigrams\n",
    "    top_20_unigrams1 = {k: v for k, v in sorted(unigrams[subreddit1].items(), key=lambda item: item[1], reverse=True)[:20]}\n",
    "    top_20_unigrams2 = {k: v for k, v in sorted(unigrams[subreddit2].items(), key=lambda item: item[1], reverse=True)[:20]}\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.bar(top_20_unigrams1.keys(), top_20_unigrams1.values(), label=subreddit1)\n",
    "    plt.bar(top_20_unigrams2.keys(), top_20_unigrams2.values(), label=subreddit2)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Top 20 Unigrams for {subreddit1} and {subreddit2}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image\n",
    "    #plt.tight_layout(pad=0)\n",
    "    #plt.savefig(f\"analysis/1a/plots/{subreddit1}_{subreddit2}_comments_unigrams_top20.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bigrams\n",
    "bigrams = {}\n",
    "for subreddit in subreddits:\n",
    "    bigrams[subreddit] = {}\n",
    "    for line, file_bytes_processed in read_lines_zst(f\"analysis/1a/{subreddit}_comments_bigrams.zst\"):\n",
    "        obj = json.loads(line)\n",
    "        bigrams[subreddit][tuple(obj[\"term\"])] = obj[\"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total frequency of bigrams in each subreddit\n",
    "total_bigrams = {}\n",
    "for subreddit in subreddits:\n",
    "    total_bigrams[subreddit] = sum(bigrams[subreddit].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bigrams = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conservative: 0.00011325\n",
      "progressive: 0.00007762\n",
      "democrats: 0.00007669\n",
      "Republican: 0.00010815\n",
      "NeutralPolitics: 0.00003501\n",
      "PoliticalDiscussion: 0.00006631\n",
      "politics: 0.00006496\n"
     ]
    }
   ],
   "source": [
    "for s in subreddits:\n",
    "    f = bigrams[s][(\"dont\", \"agree\")]\n",
    "    print(f\"{s}: {f/total_bigrams[s]:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "b_1, b_2 = (\"i\", \"argue\")\n",
    "for s in subreddits:\n",
    "    f = bigrams[s][(b_1, b_2)]\n",
    "    f_total = f/total_bigrams[s]\n",
    "    result[s] = f_total\n",
    "\n",
    "# Plot result using seaborn\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Get the x and y values sorted by x\n",
    "y = sorted(result.keys(), key=lambda k: result[k], reverse=True)\n",
    "x = [result[k] for k in y]\n",
    "\n",
    "# Plot the bar chart horizontally using seaborn. Use yellowgreen as the color with gradient\n",
    "sns.barplot(x=x, y=y, palette=\"YlGnBu_r\")\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title(f\"Frequency of Bigram ({b_1}, {b_2}) in Subreddits\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "\n",
    "# Save the image\n",
    "plt.savefig(f\"analysis/1a/plots/{b_1}_{b_2}_bigram_frequency.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Creating a network of subreddits\n",
    "\n",
    "Find users that post in multiple subreddits. Create a network of subreddits based on the users that post in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_users(file: str) -> list:\n",
    "    \"\"\"Get unique users from a file\"\"\"\n",
    "\n",
    "    # Initialize set\n",
    "    users = set()\n",
    "\n",
    "    # Read file line by line\n",
    "    with open(file, 'r') as f:\n",
    "\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # Iterate through lines, skipping header\n",
    "        for line in lines[1:]:\n",
    "                            \n",
    "            # Get user from line\n",
    "            user = line.split(',')[0]\n",
    "\n",
    "            # Skip if user is deleted or AutoModerator\n",
    "            if (user == '[deleted]') or (user == 'AutoModerator'):\n",
    "                continue\n",
    "\n",
    "            # Add user to set\n",
    "            users.add(user)\n",
    "\n",
    "    return users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary to store unique users for each subreddit\n",
    "unique_users = {}\n",
    "\n",
    "# Loop through all files to get unique users\n",
    "for file in files:\n",
    "\n",
    "    # Get unique users for each file\n",
    "    print(f'Getting unique users for {file}')\n",
    "    users = get_unique_users(file, start_date, end_date)\n",
    "\n",
    "    # Save to dictionary\n",
    "    subreddit = file.split('/')[-1].split('_')[0]\n",
    "    unique_users[subreddit] = users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c. Topic Modeling\n",
    "\n",
    "Find main topics being discussed in the subreddits.\n",
    "\n",
    "* Text preprocessing: Clean and preprocess the textual data by removing stop words, punctuation, and special characters. Perform stemming or lemmatization to normalize the text.\n",
    "\n",
    "* Topic modeling: Apply topic modeling techniques such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF) to extract the main topics discussed within each subreddit. This will help you identify the prevalent themes and subjects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_submissions = [f\"data/{s}/{s}_submissions_clean.zst\" for s in subreddits]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation (LDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_submission(text: str, stop_words: list) -> str:\n",
    "    \"\"\"Clean text by removing non-alphabetical characters, stop words,\n",
    "    and other words\"\"\"\n",
    "\n",
    "    # Remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Remove 's \n",
    "    text = text.replace(\"'s \", ' ')\n",
    "\n",
    "    # Remove non-alphabetical characters\n",
    "    text = re.sub(r'[^a-zA-Z ]+', '', text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find topics across multiple texts using LDA model\n",
    "def create_corpus(input_paths: list) -> None:\n",
    "    \"\"\"Find topics across multiple texts using LDA model\"\"\"\n",
    "\n",
    "    # Create empty list to store texts\n",
    "    texts = []\n",
    "\n",
    "    # Load stop words using nltk\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Custom stop words\n",
    "    custom_stop_words = ['biden', 'trump', 'republican', 'democrat', 'politics']\n",
    "    stop_words.extend(custom_stop_words)\n",
    "\n",
    "    # Loop through input paths\n",
    "    for path in input_paths:\n",
    "\n",
    "        # Read lines\n",
    "        lines = read_lines_zst(path)\n",
    "\n",
    "        # Loop through lines\n",
    "        for line, _ in lines:\n",
    "\n",
    "            # Convert the line to a json object\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Get text and title\n",
    "            text = obj['selftext']\n",
    "            title = obj['title']\n",
    "\n",
    "            # Skip if text is deleted, or removed\n",
    "            if (text == 'deleted') or (text == 'removed'):\n",
    "                continue\n",
    "\n",
    "            # Combine title and text\n",
    "            full_text = title + ' ' + text\n",
    "\n",
    "            # Clean text\n",
    "            full_text = clean_submission(full_text, stop_words)\n",
    "\n",
    "            # Skip if text is empty\n",
    "            if len(full_text) == 0:\n",
    "                continue\n",
    "\n",
    "            # Add to list\n",
    "            texts.append(full_text)\n",
    "\n",
    "    # Tokenize texts\n",
    "    tokenized_texts = [list(tokenize(text, lowercase=True)) for text in texts]\n",
    " \n",
    "    # Create dictionary\n",
    "    dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "    # Filter extremes (remove words that appear in more than 30% of documents and less than 10 documents)\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.3)\n",
    "\n",
    "    # Create corpus\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "    return corpus, dictionary, tokenized_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "corpus, dictionary, tokenized_texts = create_corpus(input_submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal number of topics using coherence scores\n",
    "def find_optimal_num_topics(corpus: list, dictionary: Dictionary, texts: list, limit: int, start: int=2, step: int=3) -> None:\n",
    "    \"\"\"Find the optimal number of topics\"\"\"\n",
    "\n",
    "    # Create empty list to store models\n",
    "    models = []\n",
    "\n",
    "    # Create empty list to store coherence scores\n",
    "    coherence_scores = []\n",
    "\n",
    "    # Loop through number of topics\n",
    "    for num_topics in range(start, limit, step):\n",
    "\n",
    "        # Create model\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "        # Save model\n",
    "        models.append(model)\n",
    "\n",
    "        # Create coherence model\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "        # Save coherence score\n",
    "        coherence_scores.append(coherence_model.get_coherence())\n",
    "\n",
    "    # Create dataframe of coherence scores\n",
    "    df = pd.DataFrame({'num_topics': range(start, limit, step), 'coherence_score': coherence_scores})\n",
    "\n",
    "    return models, coherence_scores, df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics\n",
    "lda_models, lda_coherence_scores, lda_df = find_optimal_num_topics(corpus, dictionary, tokenized_texts, limit=10, start=4, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_SAVE_1c = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SAVE_1c:\n",
    "     \n",
    "    # Save results\n",
    "    lda_df.to_csv('analysis/1c/lda_coherence_scores.csv', index=False)\n",
    "\n",
    "    # Save models\n",
    "    for i, model in enumerate(lda_models):\n",
    "        model.save(f'analysis/1c/lda_model_{i+3}.model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical Dirichlet Process (HDP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import HdpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HDP model\n",
    "#hdp_model = HdpModel(corpus=corpus, id2word=dictionary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify submissions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lda model with highest coherence score\n",
    "optimal_lda_model = LdaModel.load('analysis/1c/lda_model_7.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: house, court, supreme, white, news, committee, judge, state, georgia, senate\n",
      "Topic: 1 \n",
      "Words: jan, says, s, us, desantis, election, maralago, donald, fbi, joe\n",
      "Topic: 2 \n",
      "Words: us, ukraine, covid, arizona, war, says, health, years, china, care\n",
      "Topic: 3 \n",
      "Words: new, bill, s, texas, states, law, tax, governor, plan, state\n",
      "Topic: 4 \n",
      "Words: s, senate, vote, capitol, gop, says, race, doj, us, general\n",
      "Topic: 5 \n",
      "Words: s, abortion, election, gop, voting, voters, poll, climate, party, ic\n",
      "Topic: 6 \n",
      "Words: people, one, dont, opinion, time, like, student, right, get, president\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in optimal_lda_model.show_topics(formatted=False, num_words=10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, ', '.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to classify submissions using lda model\n",
    "def classify_submissions(input_paths: list,\n",
    "                         output_paths: list,\n",
    "                         lda_model: LdaModel,\n",
    "                         dictionary: Dictionary) -> None:\n",
    "    \"\"\"Classify submissions using lda model\"\"\"\n",
    "\n",
    "    # Loop through input paths\n",
    "    for in_path, out_path in zip(input_paths, output_paths):\n",
    "\n",
    "        # Create the zst handler\n",
    "        handle = zstandard.ZstdCompressor().stream_writer(open(out_path, 'wb'))\n",
    "\n",
    "        # Save the data to zst file\n",
    "        with open(out_path, mode=\"w\", newline=\"\") as file:\n",
    "\n",
    "            for line, file_bytes_processed in read_lines_zst(in_path):\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Get text and title\n",
    "                text = obj['selftext']\n",
    "                title = obj['title']\n",
    "\n",
    "                # Skip if text is deleted, or removed\n",
    "                if (text == 'deleted') or (text == 'removed'):\n",
    "                    continue\n",
    "\n",
    "                # Combine title and text\n",
    "                full_text = title + ' ' + text\n",
    "\n",
    "                # Skip if text is empty\n",
    "                if len(full_text) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Get topic distribution\n",
    "                topic_dist = lda_model.get_document_topics(dictionary.doc2bow(full_text.split()), minimum_probability=0.0)\n",
    "\n",
    "                # Add topic distribution to object (make it serializable)\n",
    "                obj['topic_dist'] = str(topic_dist)\n",
    "                \n",
    "                # Write the data to the zst file\n",
    "                new_line = json.dumps(obj)\n",
    "                write_line_zst(handle, new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submissions = [f\"data/{s}/{s}_submissions_classified.zst\" for s in subreddits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify submissions\n",
    "classify_submissions(input_submissions, output_submissions, optimal_lda_model, dictionary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d. Sentiment Analysis\n",
    "\n",
    "Assess the homogeneity of the discussions within each subreddit.\n",
    "\n",
    "* Sentiment analysis: measure the sentiment (positive, negative, or neutral) of (most engaged) comments on a specific post in each subreddit.\n",
    "Then, aggregate those sentiments, using upvotes and downvotes as weights, to get a sentiment score for each post. Finally, aggregate the sentiment scores of all posts in a subreddit to get a sentiment score for each subreddit on a specific topic.\n",
    "(consider using sentiment entropy)\n",
    "\n",
    "* Visualization and interpretation: Visualize the overall sentiment towards the topic using charts, histograms, or other visual representations. Analyze the results to interpret the subreddit's sentiment and understand the prevailing sentiment towards the topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average sentiment towards each submission \n",
    "def calculate_sentiment_submission(input_comments: list, output_comments: list) -> None:\n",
    "    \"\"\"Calculate average sentiment towards each submission\"\"\"\n",
    "\n",
    "    # Loop through input paths\n",
    "    for in_comment, out_comment in zip(input_comments, output_comments):\n",
    "\n",
    "        # Create the zst handler\n",
    "        handle = zstandard.ZstdCompressor().stream_writer(open(out_comment, 'wb'))\n",
    "\n",
    "        # Save the data to zst file\n",
    "        with open(out_comment, mode=\"w\", newline=\"\") as file:\n",
    "\n",
    "            submissions = {}\n",
    "\n",
    "            for line, file_bytes_processed in read_lines_zst(in_comment):\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Skip if body doesn't exist\n",
    "                if 'sentiment' not in obj or 'link_id' not in obj:\n",
    "                    continue\n",
    "\n",
    "                # Calculate score if ups and downs exist\n",
    "                if obj.get('ups', '0') != '0' or obj.get('downs', '0') != '0':\n",
    "\n",
    "                    # Give it a higher weight the more votes it has\n",
    "                    votes = obj['ups'] + obj['downs']\n",
    "                    if votes == 0: interactions = 1\n",
    "                    sentiment = obj['sentiment'] * votes\n",
    "                    interactions = obj['ups'] + abs(obj['downs'])\n",
    "                    \n",
    "                # Otherwise, use the score\n",
    "                else:\n",
    "                    interactions = max(obj['score'], 1)\n",
    "                    sentiment = obj['sentiment'] * interactions\n",
    "                \n",
    "                # Add sentiment to object\n",
    "                if obj['link_id'] not in submissions:\n",
    "                    submissions[obj['link_id']] = [sentiment, interactions]\n",
    "                else:\n",
    "                    submissions[obj['link_id']][0] += sentiment\n",
    "                    submissions[obj['link_id']][1] += interactions\n",
    "\n",
    "            # Save the data to zst file\n",
    "            for link_id, (sentiment, interactions) in submissions.items():\n",
    "                obj = {'link_id': link_id,\n",
    "                       'sentiment': sentiment,\n",
    "                       'interactions': interactions}\n",
    "                new_line = json.dumps(obj)\n",
    "                write_line_zst(handle, new_line)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and output paths\n",
    "output_overall_sentiment = [f\"analysis/1d/{s}_submissions_overall_sentiment.zst\" for s in subreddits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sentiment_submission(output_comments, output_overall_sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compare Echo Chambers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. Analyzing sentiment and opposing views:\n",
    "\n",
    "- Sentiment analysis: Utilize sentiment analysis techniques to determine the sentiment (positive, negative, or neutral) of the posts and comments related to specific topics within each subreddit.\n",
    "\n",
    "- Identify opposing subreddits: Identify subreddits with opposing views by comparing the sentiment and language used in their discussions.\n",
    "\n",
    "- Sentiment analysis across subreddits: Compare the sentiment distribution and polarity scores of the same topic discussed in different subreddits with opposing views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comments_sentiment = [f\"data/{s}/{s}_comments_sentiment.zst\" for s in subreddits if s != 'politics']\n",
    "output_submission_sentiment = [f\"analysis/1d/{s}_submissions_sentiment.zst\" for s in subreddits if s != 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate average emotion towards each submission\n",
    "def calculate_emotion_submission(input_comments_sentiment: list, output_submission_sentiment: list) -> None:\n",
    "    \n",
    "    emotions = {'fear': 0, 'anger': 0, 'anticip': 0, 'trust': 0,\n",
    "            'surprise': 0, 'sadness': 0, 'joy': 0, 'disgust': 0,\n",
    "            'positive': 0, 'negative': 0}\n",
    "\n",
    "    submissions = {}\n",
    "\n",
    "    # Loop through input paths\n",
    "    for in_comment, out_submission in zip(input_comments_sentiment, output_submission_sentiment):\n",
    "\n",
    "        # Create the zst handler\n",
    "        handle = zstandard.ZstdCompressor().stream_writer(open(out_submission, 'wb'))\n",
    "\n",
    "        # Save the data to zst file\n",
    "        with open(out_submission, mode=\"w\", newline=\"\") as file:\n",
    "\n",
    "            for line, file_bytes_processed in read_lines_zst(in_comment):\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Skip if body doesn't exist\n",
    "                if 'link_id' not in obj:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate score if ups and downs exist\n",
    "                if obj.get('ups', '') == '': ups = 0\n",
    "                else: ups = int(obj.get('ups', ''))\n",
    "                if obj.get('downs', '') == '': downs = 0\n",
    "                else: downs = int(obj.get('downs', ''))\n",
    "\n",
    "                if ups != 0 or downs != 0:\n",
    "                    \n",
    "                    interactions = ups + downs\n",
    "                    if interactions == 0: interactions = 1\n",
    "                    \n",
    "                # Otherwise, use the score\n",
    "                else:\n",
    "                    interactions = max(obj['score'], 1)\n",
    "                \n",
    "                # Add polarity, subjectivity, and emotions to object\n",
    "                if obj['link_id'] not in submissions:\n",
    "                    submissions[obj['link_id']] = {'polarity': 0,\n",
    "                                                   'subjectivity': 0,\n",
    "                                                   'emotions': emotions.copy(),\n",
    "                                                   'interactions': 0}\n",
    "                \n",
    "                # Calculate average polarity, subjectivity, and emotions\n",
    "                submissions[obj['link_id']]['polarity'] += obj['polaritiy'] * interactions\n",
    "                submissions[obj['link_id']]['subjectivity'] += obj['subjectivity'] * interactions\n",
    "\n",
    "                for e in emotions:\n",
    "                    submissions[obj['link_id']]['emotions'][e] += obj['emotions'][e] * interactions\n",
    "                \n",
    "                # Add interactions\n",
    "                submissions[obj['link_id']]['interactions'] += interactions\n",
    "\n",
    "        # Save the data to zst file, including link_id\n",
    "        for link_id, obj in submissions.items():\n",
    "            obj['link_id'] = link_id\n",
    "            new_line = json.dumps(obj)\n",
    "            write_line_zst(handle, new_line)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_emotion_submission(input_comments_sentiment, output_submission_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submissions_classified = [f\"data/{s}/{s}_submissions_classified.zst\" for s in subreddits if s != 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For list of tuples, find tuple with highest value in second element and return first element\n",
    "def find_max_tuple(tuples: list) -> tuple:\n",
    "    \"\"\"Find tuple with highest value in second element\"\"\"\n",
    "\n",
    "    # Initialize max tuple\n",
    "    max_tuple = ('', 0)\n",
    "\n",
    "    # Loop through tuples\n",
    "    for t in tuples:\n",
    "\n",
    "        # Update max tuple if second element is greater than current max\n",
    "        if t[1] > max_tuple[1]:\n",
    "            max_tuple = t\n",
    "\n",
    "    return max_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions classified and get topic with highest probability\n",
    "submissions_classified = {s: {} for s in subreddits if s != 'politics'}\n",
    "\n",
    "for sub in subreddits[:-1]:\n",
    "\n",
    "    s_class = f\"data/{sub}/{sub}_submissions_classified.zst\"\n",
    "\n",
    "    # Load topic distribution from each submission classified\n",
    "    for line, file_bytes_processed in read_lines_zst(s_class):\n",
    "\n",
    "            # Convert the line to a json object\n",
    "            obj = json.loads(line)\n",
    "    \n",
    "            # Get link_id\n",
    "            link_id = obj['id']\n",
    "\n",
    "            # Find topic with highest probability\n",
    "            topic_dist = ast.literal_eval(obj['topic_dist'])\n",
    "            topic = find_max_tuple(topic_dist)[0]\n",
    "\n",
    "            # Add to dictionary\n",
    "            submissions_classified[sub][link_id] = {'topic': topic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions sentiment and get average sentiment\n",
    "submissions_sentiment = {s: {} for s in subreddits if s != 'politics'}\n",
    "\n",
    "for sub in subreddits[:-1]:\n",
    "    \n",
    "    s_sent = f\"analysis/1d/{sub}_submissions_sentiment.zst\"\n",
    "\n",
    "    # Load topic distribution from each submission classified\n",
    "    for line, file_bytes_processed in read_lines_zst(s_sent):\n",
    "\n",
    "            # Convert the line to a json object\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Get link_id\n",
    "            link_id = obj['link_id'].split('_')[-1]\n",
    "\n",
    "            # Get emotions, polarity, and subjectivity\n",
    "            emotions = obj['emotions']\n",
    "            polarity = obj['polarity']\n",
    "            subjectivity = obj['subjectivity']\n",
    "\n",
    "            # Add to dictionary\n",
    "            submissions_sentiment[sub][link_id] = {'emotions': emotions,\n",
    "                                                    'polarity': polarity,\n",
    "                                                    'subjectivity': subjectivity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all submissions_classified in each subreddit, and calcuate average sentiment and emotions for each topic\n",
    "submissions_classified_sentiment = {s: {} for s in subreddits if s != 'politics'}\n",
    "count = 0\n",
    "for sub in subreddits[:-1]:\n",
    "    count += 1\n",
    "    # Loop through submissions classified\n",
    "    for link_id, obj in submissions_classified[sub].items():\n",
    "\n",
    "        count += 1\n",
    "        if link_id not in submissions_sentiment[sub]:\n",
    "            continue\n",
    "\n",
    "        # Get topic\n",
    "        topic = obj['topic']\n",
    "\n",
    "        # Get sentiment and emotions from submissions sentiment\n",
    "        sentiment = submissions_sentiment[sub][link_id]\n",
    "        emotions = sentiment['emotions']\n",
    "        polarity = sentiment['polarity']\n",
    "        subjectivity = sentiment['subjectivity']\n",
    "\n",
    "        # Add to dictionary\n",
    "        if topic not in submissions_classified_sentiment[sub]:\n",
    "            submissions_classified_sentiment[sub][topic] = {'emotions': emotions,\n",
    "                                                            'polarity': polarity,\n",
    "                                                            'subjectivity': subjectivity,\n",
    "                                                            'count': 1}\n",
    "        else:\n",
    "            submissions_classified_sentiment[sub][topic]['emotions'] = {k: v + submissions_classified_sentiment[sub][topic]['emotions'][k] for k, v in emotions.items()}\n",
    "            submissions_classified_sentiment[sub][topic]['polarity'] += polarity\n",
    "            submissions_classified_sentiment[sub][topic]['subjectivity'] += subjectivity\n",
    "            submissions_classified_sentiment[sub][topic]['count'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, plot the positive and negative emotions for each subreddit\n",
    "topics = {0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}}\n",
    "for sub in subreddits[:-1]:\n",
    "\n",
    "    for t in range(7):\n",
    "        positive = submissions_classified_sentiment[sub][t]['emotions']['positive']\n",
    "        negative = submissions_classified_sentiment[sub][t]['emotions']['negative']\n",
    "        count = submissions_classified_sentiment[sub][t]['count']\n",
    "        topics[t][sub] = {'positive': positive/count, 'negative': negative/count}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = {0: 'Judicial System', 1: 'Political Figures & Investigations', 2: 'International Affairs', 3: 'State Government', 4: 'Federal Government', 5: 'Social Issues', 6: 'Other'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"Conservative\" to \"Rigt-Leaning\", 'democrat\" to \"Left-Leaning\", and \"NeutralPolitics\" to \"Neutral\"\n",
    "topics_renamed = {topic_mapping[k]: v for k, v in topics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove from the keys the subreddits that are not needed (progressive, Republican, PoliticalDiscussion)\n",
    "topics_renamed = {k: {k2: v2 for k2, v2 in v.items() if k2 not in ['progressive', 'Republican', 'PoliticalDiscussion']} for k, v in topics.items()}\n",
    "\n",
    "# Rename from the keys \"Conservative\" to \"Rigt-Leaning\", 'democrat\" to \"Left-Leaning\", and \"NeutralPolitics\" to \"Neutral\"\n",
    "rename_dict = {'Conservative': 'Right-Leaning', 'democrats': 'Left-Leaning', 'NeutralPolitics': 'Neutral'}\n",
    "topics_renamed = {k: {rename_dict[k2]: v2 for k2, v2 in v.items()} for k, v in topics_renamed.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Right-Leaning': {'positive': 4510.43842364532,\n",
       "   'negative': 4535.679802955665},\n",
       "  'Left-Leaning': {'positive': 521.4, 'negative': 494.23},\n",
       "  'Neutral': {'positive': 11995.953125, 'negative': 11434.296875}},\n",
       " 1: {'Right-Leaning': {'positive': 6675.825980392156,\n",
       "   'negative': 6602.651960784314},\n",
       "  'Left-Leaning': {'positive': 674.5031055900621,\n",
       "   'negative': 623.8322981366459},\n",
       "  'Neutral': {'positive': 9115.82142857143, 'negative': 9143.330357142857}},\n",
       " 2: {'Right-Leaning': {'positive': 8824.87012987013,\n",
       "   'negative': 8349.298701298701},\n",
       "  'Left-Leaning': {'positive': 340.4117647058824,\n",
       "   'negative': 321.97058823529414},\n",
       "  'Neutral': {'positive': 12648.6875, 'negative': 11405.125}},\n",
       " 3: {'Right-Leaning': {'positive': 3007.7134831460676,\n",
       "   'negative': 3071.938202247191},\n",
       "  'Left-Leaning': {'positive': 214.7704918032787,\n",
       "   'negative': 221.0983606557377},\n",
       "  'Neutral': {'positive': 12876.446808510638, 'negative': 12592.31914893617}},\n",
       " 4: {'Right-Leaning': {'positive': 3746.7380952380954,\n",
       "   'negative': 3672.8928571428573},\n",
       "  'Left-Leaning': {'positive': 442.77777777777777,\n",
       "   'negative': 420.72222222222223},\n",
       "  'Neutral': {'positive': 12534.083333333334, 'negative': 12311.666666666666}},\n",
       " 5: {'Right-Leaning': {'positive': 2276.4117647058824,\n",
       "   'negative': 3039.705882352941},\n",
       "  'Left-Leaning': {'positive': 57.333333333333336,\n",
       "   'negative': 57.166666666666664},\n",
       "  'Neutral': {'positive': 1291.0, 'negative': 1616.5}},\n",
       " 6: {'Right-Leaning': {'positive': 3740.8626262626262,\n",
       "   'negative': 3516.3050505050505},\n",
       "  'Left-Leaning': {'positive': 519.6190476190476,\n",
       "   'negative': 478.67724867724866},\n",
       "  'Neutral': {'positive': 10345.585074626866, 'negative': 10406.226865671642}}}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Government\n",
      "Social Issues\n"
     ]
    }
   ],
   "source": [
    "# Plot each topic as a bar chart\n",
    "for t in range(6):\n",
    "\n",
    "    values = topics_renamed[t]\n",
    "    df = pd.DataFrame(values).T\n",
    "    df[\"overall\"] = df[\"positive\"] - df[\"negative\"]\n",
    "\n",
    "    # Plot the bar chart using seaborn\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    # Plot positive and negative emotions as bar chart horizontally using seaborn\n",
    "    sns.barplot(y=df[\"overall\"], x=df.index, palette=\"YlGnBu_r\")\n",
    "\n",
    "    # Set the title and axis labels\n",
    "    title = f\"Average Positive and Negative Emotions for Topic: {topic_mapping[t]}\"\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Average Emotion\")\n",
    "    plt.xlabel(\"Subreddits by Polical Leaning\")\n",
    "\n",
    "    # Save the image\n",
    "    plt.tight_layout(pad=1)\n",
    "    plt.savefig(f\"analysis/1d/plots/topic_{t}_positive_negative_emotions.png\")\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Ideas !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible research questions:\n",
    "\n",
    "Does the echo chamber effect have a stronger influence on certain topics?\n",
    "What are the dominant topics, sentiments, and differences in language usage between liberal and conservative subreddits, and how do these findings contribute to our understanding of echo chambers?\n",
    "\n",
    "1. Detect subreddits that act as echo chambers in Reddit:\n",
    "   - Get Reddit data (posts and comments) using the Reddit API or any available dataset.\n",
    "   - Identify different subreddits that can be good for the analysis.\n",
    "   - Measure the homogeneity of the discussions within each subreddit by looking at the similarity of language and opinions.\n",
    "\n",
    "2. Find the main topics discussed in those subreddits:\n",
    "   - Clean the textual data by removing stop words, punctuation, and special characters. Maybe stemming or lemmatization too?\n",
    "   - Identify main topics discussed in those subreddits using Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF).\n",
    "\n",
    "3. Analyze sentiment:\n",
    "   - Determine the sentiment (positive, negative, or neutral) of the posts and comments related to specific topics within each subreddit.\n",
    "   - Identify subreddits with opposing views by comparing the sentiment and language used in their discussions.\n",
    "   - (Optional) Analyze a more sentiment on a more granular level by analyzing different emotions (anger, joy, sadness, etc.)\n",
    "\n",
    "4. Analyzing sentiment change, vocabulary, and opinion shift:\n",
    "   - I also would to see if spending a greater deal of time on a subreddit that acts as an echo chamber changes their sentiment from negative/neutral to positive (or vice versa) on specific topics.\n",
    "   - Finally, I want to measure if an user's sentiment changes based on the subreddit they are interacting with -> VERY difficult to do without being able to track users consistently across subreddits and looking at exposure\n",
    "   to the same topics/comments and measuring their sentiment towards them.\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
