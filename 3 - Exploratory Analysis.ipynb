{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, json, os, re, zstandard\n",
    "from nltk.corpus import stopwords\n",
    "from zst_reader import read_lines_zst, write_line_zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\"Conservative\", \"progressive\",\n",
    "              \"democrats\", \"Republican\",\n",
    "              \"NeutralPolitics\", \"PoliticalDiscussion\", \"politics\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Echo Chambers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Unigrams and Bigrams Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms frequencies (unigram and bigram) in the comments\n",
    "def count_terms_frequency(input_comments: list, output_frequencies: list) -> None:\n",
    "\n",
    "    # Load stop words using nltk\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Loop through input paths\n",
    "    for in_comment, out_grams in zip(input_comments, output_frequencies):\n",
    "\n",
    "        unigrams = Counter()\n",
    "        bigrams = Counter()\n",
    "\n",
    "        for line, file_bytes_processed in read_lines_zst(in_comment):\n",
    "\n",
    "            # Load the json object\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Skip if body doesn't exist\n",
    "            if 'body' not in obj:\n",
    "                continue\n",
    "\n",
    "            # Get body of comment\n",
    "            body = obj['body']\n",
    "\n",
    "            # Clean the text\n",
    "            body = clean_comments(body, stop_words)\n",
    "\n",
    "            # Split the text into unigrams and bigrams\n",
    "            unigrams_list = body.split()\n",
    "            bigrams_list = list(ngrams(unigrams_list, 2))\n",
    "\n",
    "            unigrams.update(unigrams_list)\n",
    "            bigrams.update(bigrams_list)\n",
    "\n",
    "        # Create the zst handler\n",
    "        handle_unigram = zstandard.ZstdCompressor().stream_writer(open(out_grams[0], 'wb'))\n",
    "\n",
    "        # Write the unigrams to the zst file\n",
    "        for unigram in unigrams:\n",
    "            line = {'term': unigram, 'frequency': unigrams[unigram]}\n",
    "            line = json.dumps(line)\n",
    "            write_line_zst(handle_unigram, line)\n",
    "\n",
    "        # Create the zst handler\n",
    "        handle_bigram = zstandard.ZstdCompressor().stream_writer(open(out_grams[1], 'wb'))\n",
    "\n",
    "        # Write the bigrams to the zst file\n",
    "        for bigram in bigrams:\n",
    "            line = {'term': bigram, 'frequency': bigrams[bigram]}\n",
    "            line = json.dumps(line)\n",
    "            write_line_zst(handle_bigram, line)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comments = [f\"data/{s}/{s}_comments_clean.zst\" for s in subreddits]\n",
    "output_frequencies = [(f\"analysis/ngrams/{s}_comments_unigrams.zst\", f\"analysis/ngrams/{s}_comments_bigrams.zst\") for s in subreddits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_terms_frequency(input_comments, output_frequencies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unigrams\n",
    "unigrams = {}\n",
    "for subreddit in subreddits:\n",
    "    unigrams[subreddit] = {}\n",
    "    for line, file_bytes_processed in read_lines_zst(f\"analysis/ngrams/{subreddit}_comments_unigrams.zst\"):\n",
    "        obj = json.loads(line)\n",
    "        unigrams[subreddit][obj[\"term\"]] = obj[\"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words = [re.sub(\"'\", \"\", word) for word in stop_words]\n",
    "stop_words += [\"like\", \"ever\", \"ive\", \"always\", \"final\", \"people\", \"would\", \"rrepublican\", \"rneutralpolitics\", \"get\", \"one\", \"thats\", \"trump\", \"karma\", \"said\"]\n",
    "# Function to clean unigrams\n",
    "def clean_unigrams(unigrams: dict, min_frequency: int=10, max_frequency: int=1000) -> dict:\n",
    "\n",
    "    # Strip whitespace\n",
    "    unigrams = {k.strip(): v for k, v in unigrams.items()}\n",
    "    \n",
    "    # Remove stop words\n",
    "    unigrams = {k: v for k, v in unigrams.items() if k not in stop_words}\n",
    "    \n",
    "    # Remove terms with frequency less than min_frequency\n",
    "    unigrams = {k: v for k, v in unigrams.items() if v >= min_frequency}\n",
    "\n",
    "    # Remove terms with frequency more than max_frequency\n",
    "    unigrams = {k: v for k, v in unigrams.items() if v <= max_frequency}\n",
    "\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_clean = {}\n",
    "for subreddit in subreddits:\n",
    "    unigrams_clean[subreddit] = clean_unigrams(unigrams[subreddit], min_frequency=10, max_frequency=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unigram frequencies for each subreddit\n",
    "unigrams_frequencies = {}\n",
    "for subreddit in subreddits:\n",
    "    total = sum(unigrams[subreddit].values())\n",
    "    unigrams_frequencies[subreddit] = {k: v/total for k, v in unigrams[subreddit].items()}\n",
    "\n",
    "    # Remove stop words\n",
    "    unigrams_frequencies[subreddit] = {k: v for k, v in unigrams_frequencies[subreddit].items() if k not in stop_words}\n",
    "\n",
    "    # Remove terms with frequency in the top 1%\n",
    "    unigrams_frequencies[subreddit] = {k: v for k, v in unigrams_frequencies[subreddit].items() if v < 0.002}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a word cloud for each subreddit using the top 100 unigrams\n",
    "for subreddit in subreddits:\n",
    "\n",
    "    # Create the word cloud\n",
    "    wc = WordCloud(background_color=\"white\", max_words=150, width=800, height=400)\n",
    "    wc.generate_from_frequencies(unigrams_clean[subreddit])\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for {subreddit}\")\n",
    "    #plt.show()\n",
    "\n",
    "    # Save the image\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f\"analysis/ngrams/plots/{subreddit}_comments_2_wordcloud.png\")\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart of the 20 most frequent unigrams for each subreddit\n",
    "for subreddit in subreddits:\n",
    "    \n",
    "    # Get the top 20 unigrams\n",
    "    top_20_unigrams = dict(sorted(unigrams_clean[subreddit].items(), key=lambda item: item[1], reverse=True)[:20])\n",
    "\n",
    "    # Plot the bar chart horizontally using seaborn. Use yellowgreen as the color with gradient\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.barplot(x=list(top_20_unigrams.values()), y=list(top_20_unigrams.keys()), palette=\"YlGnBu_r\")\n",
    "\n",
    "    # Set the title and axis labels\n",
    "    plt.title(f\"Top 20 Unigrams for {subreddit}\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Unigram\")\n",
    "\n",
    "    # Save the image\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f\"analysis/ngrams/plots/{subreddit}_comments_clean_unigrams_top20.png\")\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unigram frequency between Republican and Democrat subreddits of top 100 unigrams\n",
    "# Create a dataframe of the top 100 unigrams for each subreddit\n",
    "df_unigrams = pd.DataFrame(unigrams_frequencies)\n",
    "df_unigrams = df_unigrams.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conservative</th>\n",
       "      <th>progressive</th>\n",
       "      <th>democrats</th>\n",
       "      <th>Republican</th>\n",
       "      <th>NeutralPolitics</th>\n",
       "      <th>PoliticalDiscussion</th>\n",
       "      <th>politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>die</th>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>2.248652e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>5.463000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afford</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.656714e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthcare</th>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>4.594411e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aca</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>7.595825e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictespecially</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looksa</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massprivatizationof</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falsedillemma</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discussionoff</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098700e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2389907 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Conservative  progressive  democrats  Republican   \n",
       "die                      0.000354     0.000349   0.000297    0.000294  \\\n",
       "able                     0.000597     0.000572   0.000522    0.000486   \n",
       "afford                   0.000153     0.000256   0.000134    0.000125   \n",
       "healthcare               0.000283     0.000585   0.000412    0.000274   \n",
       "aca                      0.000015     0.000115   0.000081    0.000016   \n",
       "...                           ...          ...        ...         ...   \n",
       "predictespecially        0.000000     0.000000   0.000000    0.000000   \n",
       "looksa                   0.000000     0.000000   0.000000    0.000000   \n",
       "massprivatizationof      0.000000     0.000000   0.000000    0.000000   \n",
       "falsedillemma            0.000000     0.000000   0.000000    0.000000   \n",
       "discussionoff            0.000000     0.000000   0.000000    0.000000   \n",
       "\n",
       "                     NeutralPolitics  PoliticalDiscussion      politics  \n",
       "die                         0.000089             0.000216  2.248652e-04  \n",
       "able                        0.000614             0.000690  5.463000e-04  \n",
       "afford                      0.000132             0.000161  1.656714e-04  \n",
       "healthcare                  0.000296             0.000553  4.594411e-04  \n",
       "aca                         0.000073             0.000129  7.595825e-05  \n",
       "...                              ...                  ...           ...  \n",
       "predictespecially           0.000000             0.000000  2.098700e-09  \n",
       "looksa                      0.000000             0.000000  2.098700e-09  \n",
       "massprivatizationof         0.000000             0.000000  2.098700e-09  \n",
       "falsedillemma               0.000000             0.000000  2.098700e-09  \n",
       "discussionoff               0.000000             0.000000  2.098700e-09  \n",
       "\n",
       "[2389907 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatterplot of unigram frequency between two subreddits (select top 100 unigrams)\n",
    "s_1 = \"democrats\"\n",
    "s_2 = \"politics\"\n",
    "\n",
    "# Create the scatterplot\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(df_unigrams[s_1], df_unigrams[s_2], alpha=0.5)\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title(f\"Unigram Frequency Comparison between {s_1} and {s_2}\")\n",
    "plt.xlabel(f\"Frequency of {s_1}\")\n",
    "plt.ylabel(f\"Frequency of {s_2}\")\n",
    "\n",
    "# Anotate the top 10 unigrams\n",
    "for i, unigram in enumerate(df_unigrams.index):\n",
    "    if i < 10:\n",
    "        plt.annotate(unigram, (df_unigrams.loc[unigram, s_1], df_unigrams.loc[unigram, s_2]),\n",
    "                     fontweight=\"bold\", backgroundcolor=\"white\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Add a line with slope 1\n",
    "plt.plot([0, 0.002], [0, 0.002], color=\"grey\", linestyle=\"--\")\n",
    "\n",
    "plt.savefig(f\"analysis/ngrams/plots/{s_1}_{s_2}_unigram_frequency_comparison.png\")\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unigram frequency between Republican and Democrat subreddits, and between Conservative and Progressive subreddits\n",
    "for subreddit1, subreddit2 in [(\"Republican\", \"democrats\"), (\"Conservative\", \"progressive\")]:\n",
    "\n",
    "    # Get the top 20 unigrams\n",
    "    top_20_unigrams1 = {k: v for k, v in sorted(unigrams[subreddit1].items(), key=lambda item: item[1], reverse=True)[:20]}\n",
    "    top_20_unigrams2 = {k: v for k, v in sorted(unigrams[subreddit2].items(), key=lambda item: item[1], reverse=True)[:20]}\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.bar(top_20_unigrams1.keys(), top_20_unigrams1.values(), label=subreddit1)\n",
    "    plt.bar(top_20_unigrams2.keys(), top_20_unigrams2.values(), label=subreddit2)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Top 20 Unigrams for {subreddit1} and {subreddit2}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image\n",
    "    #plt.tight_layout(pad=0)\n",
    "    #plt.savefig(f\"analysis/ngrams/plots/{subreddit1}_{subreddit2}_comments_unigrams_top20.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bigrams\n",
    "bigrams = {}\n",
    "for subreddit in subreddits:\n",
    "    bigrams[subreddit] = {}\n",
    "    for line, file_bytes_processed in read_lines_zst(f\"analysis/ngrams/{subreddit}_comments_bigrams.zst\"):\n",
    "        obj = json.loads(line)\n",
    "        bigrams[subreddit][tuple(obj[\"term\"])] = obj[\"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total frequency of bigrams in each subreddit\n",
    "total_bigrams = {}\n",
    "for subreddit in subreddits:\n",
    "    total_bigrams[subreddit] = sum(bigrams[subreddit].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bigrams = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conservative: 0.00011325\n",
      "progressive: 0.00007762\n",
      "democrats: 0.00007669\n",
      "Republican: 0.00010815\n",
      "NeutralPolitics: 0.00003501\n",
      "PoliticalDiscussion: 0.00006631\n",
      "politics: 0.00006496\n"
     ]
    }
   ],
   "source": [
    "# Bigrams that show disagreement:\n",
    "# - dont agree\n",
    "# - disagree with/that/on/about\n",
    "for s in subreddits:\n",
    "    f = bigrams[s][(\"dont\", \"agree\")]\n",
    "    print(f\"{s}: {f/total_bigrams[s]:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "b_1, b_2 = (\"i\", \"argue\")\n",
    "for s in subreddits:\n",
    "    f = bigrams[s][(b_1, b_2)]\n",
    "    f_total = f/total_bigrams[s]\n",
    "    result[s] = f_total\n",
    "\n",
    "# Plot result using seaborn\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Get the x and y values sorted by x\n",
    "y = sorted(result.keys(), key=lambda k: result[k], reverse=True)\n",
    "x = [result[k] for k in y]\n",
    "\n",
    "# Plot the bar chart horizontally using seaborn. Use yellowgreen as the color with gradient\n",
    "sns.barplot(x=x, y=y, palette=\"YlGnBu_r\")\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title(f\"Frequency of Bigram ({b_1}, {b_2}) in Subreddits\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "\n",
    "# Save the image\n",
    "plt.savefig(f\"analysis/ngrams/plots/{b_1}_{b_2}_bigram_frequency.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Users and Subreddits\n",
    "\n",
    "Find users that post in multiple subreddits. Create a network of subreddits based on the users that post in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_users(file: str) -> list:\n",
    "    \"\"\"Get unique users from a file\"\"\"\n",
    "\n",
    "    # Initialize set\n",
    "    users = set()\n",
    "\n",
    "    # Read file line by line\n",
    "    with open(file, 'r') as f:\n",
    "\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # Iterate through lines, skipping header\n",
    "        for line in lines[1:]:\n",
    "                            \n",
    "            # Get user from line\n",
    "            user = line.split(',')[0]\n",
    "\n",
    "            # Skip if user is deleted or AutoModerator\n",
    "            if (user == '[deleted]') or (user == 'AutoModerator'):\n",
    "                continue\n",
    "\n",
    "            # Add user to set\n",
    "            users.add(user)\n",
    "\n",
    "    return users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary to store unique users for each subreddit\n",
    "unique_users = {}\n",
    "\n",
    "# Loop through all files to get unique users\n",
    "for file in files:\n",
    "\n",
    "    # Get unique users for each file\n",
    "    print(f'Getting unique users for {file}')\n",
    "    users = get_unique_users(file, start_date, end_date)\n",
    "\n",
    "    # Save to dictionary\n",
    "    subreddit = file.split('/')[-1].split('_')[0]\n",
    "    unique_users[subreddit] = users"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
